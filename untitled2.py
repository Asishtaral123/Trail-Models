# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fZk9137Hi7-imSoQP-zsProhMK6iBZ5R
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

# Step 2: Load the dataset
data = pd.read_csv("/content/soil-data (3).csv")

# Step 3: Preprocess the data
# Encoding categorical variables
label_encoder = LabelEncoder()
data['Crop_Type'] = label_encoder.fit_transform(data['Crop_Type'])
data['Fertilizer_Type'] = label_encoder.fit_transform(data['Fertilizer_Type'])

# Step 3 : Scale numerical features
scaler = MinMaxScaler()
numerical_features = ['Temperature', 'Humidity', 'Soil_Moisture', 'Raindrop', 'Soil_pH', 'Soil_Nitrogen', 'Soil_Phosphorus', 'Soil_Potassium']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

# Splitting features and target variables
X = data.drop(['Crop_Type', 'Fertilizer_Type'], axis=1)
y_crop = data['Crop_Type']
y_fertilizer = data['Fertilizer_Type']

# Step 4: Split the data into training and testing sets
X_train_crop, X_test_crop, y_train_crop, y_test_crop = train_test_split(X, y_crop, test_size=0.2, random_state=42)
X_train_fert, X_test_fert, y_train_fert, y_test_fert = train_test_split(X, y_fertilizer, test_size=0.2, random_state=42)

# Define parameter grids for KNN and Bagging classifiers
knn_param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}
bagging_param_grid = {'n_estimators': [5, 10, 15, 20]}

# Perform grid search for KNN classifier
knn_grid_search = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5)
knn_grid_search.fit(X_train_crop, y_train_crop)

# Perform grid search for Bagging classifier
bagging_grid_search = GridSearchCV(BaggingClassifier(base_estimator=KNeighborsClassifier()), bagging_param_grid, cv=7)
bagging_grid_search.fit(X_train_crop, y_train_crop)

# Get the best hyperparameters
best_knn_params = knn_grid_search.best_params_
best_bagging_params = bagging_grid_search.best_params_

print("Best KNN parameters:", best_knn_params)
print("Best Bagging parameters:", best_bagging_params)

# Initialize KNN and Bagging classifiers with best parameters
best_knn_classifier = KNeighborsClassifier(**best_knn_params)
best_bagging_classifier = BaggingClassifier(base_estimator=KNeighborsClassifier(**best_knn_params), **best_bagging_params)

# Train the models with best hyperparameters
best_knn_classifier.fit(X_train_crop, y_train_crop)
best_bagging_classifier.fit(X_train_crop, y_train_crop)

# Evaluate the models
y_pred_crop_knn = best_knn_classifier.predict(X_test_crop)
y_pred_crop_bagging = best_bagging_classifier.predict(X_test_crop)

accuracy_crop_knn = accuracy_score(y_test_crop, y_pred_crop_knn)
accuracy_crop_bagging = accuracy_score(y_test_crop, y_pred_crop_bagging)

print("Accuracy of KNN classifier:", accuracy_crop_knn)
print("Accuracy of Bagging classifier:", accuracy_crop_bagging)

from sklearn.ensemble import VotingClassifier

# Create a voting classifier combining KNN and Bagging
voting_classifier = VotingClassifier(
    estimators=[('knn', best_knn_classifier), ('bagging', best_bagging_classifier)],
    voting='hard'
)

# Train the voting classifier
voting_classifier.fit(X_train_crop, y_train_crop)

# Predict using the voting classifier
y_pred_crop_voting = voting_classifier.predict(X_test_crop)

# Evaluate the voting classifier
accuracy_crop_voting = accuracy_score(y_test_crop, y_pred_crop_voting)
print("Accuracy of Voting classifier:", accuracy_crop_voting)

# Assuming 'Crop_Type' is my target variable
target_variable = 'Crop_Type'

# Compute correlation coefficients
correlation_coefficients = data.corr()[target_variable].drop(target_variable)

# Print correlation coefficients
print("Correlation coefficients with the target variable:")
print(correlation_coefficients)

# Based on the correlation coefficients between each feature and the target variable (Crop_Type), we can observe the following:

# Features with positive correlation coefficients: Unnamed: 0, Temperature, Soil_pH, Soil_Phosphorus, Soil_Potassium, and Fertilizer_Type. Positive correlation indicates that as the value of these features increases, the likelihood of a certain crop type also tends to increase.

# Features with negative correlation coefficients: Humidity and Soil_Nitrogen. Negative correlation indicates that as the value of these features increases, the likelihood of a certain crop type tends to decrease.

# Features with correlation coefficients close to zero: Soil_Moisture and Raindrop. These features have weak or negligible correlation with the target variable, indicating that they may not have much predictive power for determining the crop type.

# It's important to note that correlation does not imply causation, and other factors may influence the relationship between features and the target variable. Additionally, the strength of correlation alone may not determine the importance of a feature in predicting the target variable, so further analysis and modeling are needed to validate these findings.
# 0.8 - 1.0 (Strong positive or negative correlation)
# 0.6 - 0.8 (Moderate positive or negative correlation)
# 0.4 - 0.6 (Weak positive or negative correlation)
# 0.0 - 0.4 (Weak or negligible correlation)